{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# EN = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from general_utils import apply_parallel, flattenlist\n",
    "EN = en_core_web_sm.load()\n",
    "\n",
    "from ktext.preprocess import processor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import get_step2_prerequisite_files, read_training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def listlen(x):\n",
    "    if not isinstance(x, list):\n",
    "        return 0\n",
    "    return len(x)\n",
    "\n",
    "# separate functions w/o docstrings\n",
    "# docstrings should be at least 3 words in the docstring to be considered a valid docstring\n",
    "\n",
    "with_docstrings = df[df.docstring_tokens.str.split().apply(listlen) >= 3]\n",
    "without_docstrings = df[df.docstring_tokens.str.split().apply(listlen) < 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Partition code by repository to minimize leakage between train, valid & test sets. \n",
    "Rough assumption that each repository has its own style.  We want to avoid having code from the same repository in the training set as well as the validation or holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "grouped = with_docstrings.groupby('nwo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chintanshah/anaconda3/envs/python-ast/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# train, valid, test splits\n",
    "train, test = train_test_split(list(grouped), train_size=0.87, shuffle=True, random_state=8081)\n",
    "# train, valid = train_test_split(train, train_size=0.82, random_state=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train = pd.concat([d for _, d in train]).reset_index(drop=True)\n",
    "# valid = pd.concat([d for _, d in valid]).reset_index(drop=True)\n",
    "test = pd.concat([d for _, d in test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set num rows 1,222,687\n",
      "test set num rows 179,249\n",
      "without docstring rows 4,001,960\n"
     ]
    }
   ],
   "source": [
    "print(f'train set num rows {train.shape[0]:,}')\n",
    "# print(f'valid set num rows {valid.shape[0]:,}')\n",
    "print(f'test set num rows {test.shape[0]:,}')\n",
    "print(f'without docstring rows {without_docstrings.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Preview what the training set looks like.  You can start to see how the data looks, the function tokens and docstring tokens are what will be fed downstream into the models.  The other information is important for diagnostics and bookeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222687,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['api_sequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>lineno</th>\n",
       "      <th>original_function</th>\n",
       "      <th>function_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>api_sequence</th>\n",
       "      <th>tokenized_function_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__init__</td>\n",
       "      <td>19</td>\n",
       "      <td>def __init__(self, *leafs, **edges):\\n    self...</td>\n",
       "      <td>def __init__ self leafs edges self edges edges...</td>\n",
       "      <td></td>\n",
       "      <td>self edges edges self leafs sorted leafs</td>\n",
       "      <td>init</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__eq__</td>\n",
       "      <td>23</td>\n",
       "      <td>def __eq__(self, other):\\n    if isinstance(ot...</td>\n",
       "      <td>def __eq__ self other if isinstance other Node...</td>\n",
       "      <td></td>\n",
       "      <td>if isinstance other node return id self id oth...</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__repr__</td>\n",
       "      <td>29</td>\n",
       "      <td>def __repr__(self):\\n    return 'Node&lt;leafs={}...</td>\n",
       "      <td>def __repr__ self return Node leafs edges form...</td>\n",
       "      <td></td>\n",
       "      <td>return node&lt;leafs={}, edges={}&gt; format self le...</td>\n",
       "      <td>repr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>create_or_get</td>\n",
       "      <td>32</td>\n",
       "      <td>def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...</td>\n",
       "      <td>def createOrGet self token if token in self ed...</td>\n",
       "      <td>create or get the node pointed to by ` token `...</td>\n",
       "      <td>if token self edges node self edges token else...</td>\n",
       "      <td>create or get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>set_leaf</td>\n",
       "      <td>47</td>\n",
       "      <td>def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...</td>\n",
       "      <td>def setLeaf self key order self leafs append o...</td>\n",
       "      <td>store the ` key ` as a leaf of this node at po...</td>\n",
       "      <td>self leafs append order key self leafs sorted ...</td>\n",
       "      <td>set leaf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nwo                       path  function_name  lineno  \\\n",
       "0  fnl/libfnl  src/fnl/nlp/dictionary.py       __init__      19   \n",
       "1  fnl/libfnl  src/fnl/nlp/dictionary.py         __eq__      23   \n",
       "2  fnl/libfnl  src/fnl/nlp/dictionary.py       __repr__      29   \n",
       "3  fnl/libfnl  src/fnl/nlp/dictionary.py  create_or_get      32   \n",
       "4  fnl/libfnl  src/fnl/nlp/dictionary.py       set_leaf      47   \n",
       "\n",
       "                                   original_function  \\\n",
       "0  def __init__(self, *leafs, **edges):\\n    self...   \n",
       "1  def __eq__(self, other):\\n    if isinstance(ot...   \n",
       "2  def __repr__(self):\\n    return 'Node<leafs={}...   \n",
       "3  def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...   \n",
       "4  def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...   \n",
       "\n",
       "                                     function_tokens  \\\n",
       "0  def __init__ self leafs edges self edges edges...   \n",
       "1  def __eq__ self other if isinstance other Node...   \n",
       "2  def __repr__ self return Node leafs edges form...   \n",
       "3  def createOrGet self token if token in self ed...   \n",
       "4  def setLeaf self key order self leafs append o...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  create or get the node pointed to by ` token `...   \n",
       "4  store the ` key ` as a leaf of this node at po...   \n",
       "\n",
       "                                        api_sequence tokenized_function_name  \n",
       "0           self edges edges self leafs sorted leafs                    init  \n",
       "1  if isinstance other node return id self id oth...                      eq  \n",
       "2  return node<leafs={}, edges={}> format self le...                    repr  \n",
       "3  if token self edges node self edges token else...           create or get  \n",
       "4  self leafs append order key self leafs sorted ...                set leaf  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 227 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 42 sec\n",
      "WARNING:root:Finished parsing 1,222,687 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 34 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 69 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 13 sec\n",
      "WARNING:root:Finished parsing 1,222,687 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 17 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 5 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 37 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 4 sec\n",
      "WARNING:root:Finished parsing 1,222,687 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 11 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 45 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 154 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 32 sec\n",
      "WARNING:root:Finished parsing 1,222,687 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 33 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "keep_n = 10000\n",
    "\n",
    "# todo, probably tokens should also be seq to seq\n",
    "function_token_processor = processor(heuristic_pct_padding=0.7, keep_n=keep_n, padding='post', truncating='post')\n",
    "train_token_v = function_token_processor.fit_transform(train['function_tokens'])\n",
    "\n",
    "docstring_processor = processor(append_indicators=True, heuristic_pct_padding=0.7, keep_n=keep_n, padding='post', truncating='post')\n",
    "train_docstring_v = docstring_processor.fit_transform(train['docstring_tokens'])\n",
    "\n",
    "methname_processor = processor(append_indicators=True, heuristic_pct_padding=0.7, keep_n=keep_n, padding='post', truncating='post')\n",
    "train_methname_v = methname_processor.fit_transform(train['tokenized_function_name'])\n",
    "\n",
    "api_seq_processor = processor(append_indicators=True, heuristic_pct_padding=0.7, keep_n=keep_n, padding='post', truncating='post')\n",
    "train_api_seq_v = api_seq_processor.fit_transform(train['api_sequence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222687, 55)\n"
     ]
    }
   ],
   "source": [
    "print(train_token_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222687, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_docstring_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222687, 45)\n"
     ]
    }
   ],
   "source": [
    "print(train_api_seq_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222687, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_methname_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path('./data/vectors/processors/')\n",
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Save the preprocessor\n",
    "with open(OUTPUT_PATH/'function_token_processor.dpkl', 'wb') as f:\n",
    "    dpickle.dump(function_token_processor, f)\n",
    "\n",
    "with open(OUTPUT_PATH/'docstring_processor.dpkl', 'wb') as f:\n",
    "    dpickle.dump(docstring_processor, f)\n",
    "\n",
    "with open(OUTPUT_PATH/'methname_processor.dpkl', 'wb') as f:\n",
    "    dpickle.dump(methname_processor, f)\n",
    "\n",
    "with open(OUTPUT_PATH/'api_seq_processor.dpkl', 'wb') as f:\n",
    "    dpickle.dump(api_seq_processor, f)\n",
    "\n",
    "# # Save the processed data\n",
    "# np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "# np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "\n",
    "def save_vecs(vecs, fout):\n",
    "    np.save(fout, vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vecs(train_token_v, './data/vectors/train.tokens.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vecs(train_api_seq_v, './data/vectors/train.apiseq.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vecs(train_methname_v, './data/vectors/train.methname.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vecs(train_docstring_v, './data/vectors/train.desc.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:...tokenizing data\n",
      "WARNING:root:...indexing data\n",
      "WARNING:root:...padding data\n"
     ]
    }
   ],
   "source": [
    "test_token_v = function_token_processor.transform_parallel(test['function_tokens'])\n",
    "test_api_seq_v = api_seq_processor.transform_parallel(test['api_sequence'])\n",
    "test_methname_v = methname_processor.transform_parallel(test['tokenized_function_name'])\n",
    "test_docstring_v = docstring_processor.transform_parallel(test['docstring_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vecs(test_token_v, './data/vectors/test.tokens.npy')\n",
    "save_vecs(test_api_seq_v, './data/vectors/test.apiseq.npy')\n",
    "save_vecs(test_methname_v, './data/vectors/test.methname.npy')\n",
    "save_vecs(test_docstring_v, './data/vectors/test.desc.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/vectors/vocab.apiseq.pkl', 'wb') as f:\n",
    "    pickle.dump(api_seq_processor.token2id, f)\n",
    "\n",
    "with open('./data/vectors/vocab.methname.pkl', 'wb') as f:\n",
    "    pickle.dump(methname_processor.token2id, f)\n",
    "\n",
    "with open('./data/vectors/vocab.desc.pkl', 'wb') as f:\n",
    "    pickle.dump(docstring_processor.token2id, f)\n",
    "\n",
    "with open('./data/vectors/vocab.tokens.pkl', 'wb') as f:\n",
    "    pickle.dump(function_token_processor.token2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          @indexer(IModule) def SearchableTextIndexer(ob...\n",
       "1          def tree_depth(obj):     \"\"\"Determine how deep...\n",
       "2          def item_depth(item):     \"\"\"Return the survey...\n",
       "3          def _get_id(self, orig_id):     \"\"\"Pick an id ...\n",
       "4          def update(self):     \"\"\" Set view attributes ...\n",
       "5          def checkDepth(self):     \"\"\"Check if creating...\n",
       "6          def checkForRisks(self):     \"\"\"Check if the c...\n",
       "7          def allowed(self):     \"\"\" A module is allowed...\n",
       "8          def getToken(field, value, default=None):     ...\n",
       "9          def exportImage(self, parent, image, caption=N...\n",
       "10         def exportSurvey(self, parent, survey):     \"\"...\n",
       "11         def exportProfileQuestion(self, parent, profil...\n",
       "12         def exportModule(self, parent, module):     \"\"...\n",
       "13         def exportRisk(self, parent, risk):     \"\"\" :r...\n",
       "14         def exportSolution(self, parent, solution):   ...\n",
       "15         def render(self):     \"\"\" :returns: an XML exp...\n",
       "16         def _canCopy(self, op=0):     \"\"\"Tell Zope2 th...\n",
       "17         def XtestSkipChildrenFalseForMandatoryModules(...\n",
       "18         @grok.subscribe(ISurvey, ISurveyUnpublishEvent...\n",
       "19         def graceful_recovery(default=None, log_args=T...\n",
       "20         def addEuphorieAccountPlugin(self, id, title='...\n",
       "21         def authenticate(login, password):     \"\"\"Try ...\n",
       "22         def updateUser(self, user_id, login_name):    ...\n",
       "23         def updateEveryLoginName(self, quit_on_first_e...\n",
       "24         def validate(self, value):     \"\"\" Ensure that...\n",
       "25         def validate(self, value):     \"\"\" Ensure that...\n",
       "26         def doChangeUser(self, userid, password, **kwa...\n",
       "27         def generate_token(user):     \"\"\"Convenience u...\n",
       "28         def do_POST(self):     \"\"\"Try to authenticate ...\n",
       "29         def validate(self, value):     \"\"\"         Onl...\n",
       "                                 ...                        \n",
       "1222657    def getVol(self):     \"\"\"Returns device volume...\n",
       "1222658    def getVolPer(self):     \"\"\"Returns device vol...\n",
       "1222659    def setInput(self, input_selector):     \"\"\"Sen...\n",
       "1222660    def getInput(self):     \"\"\"Returns current inp...\n",
       "1222661    def allclose_sign(a1, a2, atol=1e-08, rtol=1e-...\n",
       "1222662    def allclose_permute(a1, a2, atol=1e-08, rtol=...\n",
       "1222663    def allclose_sign_permute(a1, a2, atol=1e-08, ...\n",
       "1222664    def is_aware(value):     \"\"\"     Determines if...\n",
       "1222665    def test_creator_plays_nice_with_module_inspec...\n",
       "1222666    def get_file_info(payload, file_metadata):    ...\n",
       "1222667    def get_dir_info(payload, dir_metadata):     \"...\n",
       "1222668    def generate_dir_metadata(d_path):     \"\"\"    ...\n",
       "1222669    def generate_file_metadata(file_path):     \"\"\"...\n",
       "1222670    def get_data(method_name, file_metadata):     ...\n",
       "1222671    def get_var_from_line(line):     \"\"\"     Searc...\n",
       "1222672    def inject_in_dir(self, dir_metadata):     \"\"\"...\n",
       "1222673    def export_payload(self):     \"\"\"         Copy...\n",
       "1222674    def set_payload_settings(self, payload_path): ...\n",
       "1222675    def get_valid_regs(returned_reg, regs):     \"\"...\n",
       "1222676    def clean_reg_name(reg, list):     \"\"\"     Cle...\n",
       "1222677    def inject(self, file_path, file_metadata):   ...\n",
       "1222678    def set_payload_settings(self, payload_path): ...\n",
       "1222679    def get_instruction(self, reg_for_stacktrace, ...\n",
       "1222680    def parse_args():     \"\"\"     Parse and valida...\n",
       "1222681    def copy(src, dest):     \"\"\"     Remove the de...\n",
       "1222682    def inject(self, file_path, file_metadata):   ...\n",
       "1222683    def set_payload_settings(self):     \"\"\"       ...\n",
       "1222684    def get_updated_AndroidManifest(self):     \"\"\"...\n",
       "1222685    def add_AndroidManifest_permissions(self, file...\n",
       "1222686    def get_main_activity(app_path):     \"\"\"     R...\n",
       "Name: original_function, Length: 1222687, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['original_function'].replace('\\n', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-ast] *",
   "language": "python",
   "name": "conda-env-python-ast-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
